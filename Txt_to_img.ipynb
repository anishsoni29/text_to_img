{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9e8a763",
   "metadata": {},
   "source": [
    "## Text to image generation using Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c48c27",
   "metadata": {},
   "source": [
    "### Installing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e234895",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers==0.3.0 --q\n",
    "!pip install transformers scipy ftfy --q\n",
    "!pip install kaggle --q\n",
    "!pip install \"ipywidgets>=7,<8\" --q\n",
    "import IPython.display "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdae6275",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cbf2cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydantic --q\n",
    "import gc\n",
    "import torch\n",
    "from PIL import Image\n",
    "import IPython.display \n",
    "from torch import autocast\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers import LMSDiscreteScheduler , PNDMScheduler\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05964bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi, list_models\n",
    "\n",
    "# Use root method\n",
    "models = list_models()\n",
    "\n",
    "# Or configure a HfApi client\n",
    "hf_api = HfApi(\n",
    "    endpoint=\"https://huggingface.co\",\n",
    "    token=\"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\", # Token is not persisted on the machine.\n",
    ")\n",
    "models = hf_api.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54c5da4",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f40d801",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config : \n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    HEIGHT = 512                        \n",
    "    WIDTH = 512                         \n",
    "    NUM_INFERENCE_STEPS = 500            \n",
    "    GUIDANCE_SCALE = 7.5                \n",
    "    GENERATOR = torch.manual_seed(48)   \n",
    "    BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f3ef82c",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9297a01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "    w,h = img[0].size\n",
    "    grid = Image.new('RGB', size = (cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14efa7e",
   "metadata": {},
   "source": [
    "### Pipeline -- loading the pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70bf73ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_auth_token=hf_api.token)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_auth_token=hf_api.token)\n",
    "vae = vae.to(config.DEVICE)\n",
    "text_encoder = text_encoder.to(config.DEVICE)\n",
    "unet = unet.to(config.DEVICE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facec76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\033[94mTokenizer, Text Encoder, VAE, Unet are loaded !!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efe0e0f",
   "metadata": {},
   "source": [
    "### Scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25d27848",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = LMSDiscreteScheduler(beta_start=0.00085, beta_end=0.012, beta_schedule=\"scaled_linear\", num_train_timesteps=1000)\n",
    "print(f'\\033[94mThe scheduler loaded is K-LMS Sceheduler')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2adaf42",
   "metadata": {},
   "source": [
    "### Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859afb4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"2 tigers and 1 black jaguar fighting for a bunch of apples in amazon rainforest with various animals such as elephants, zebras and hyenas looking at them\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a735785",
   "metadata": {},
   "source": [
    "### Setting the embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b2b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "with torch.no_grad():\n",
    "      text_embeddings = text_encoder(text_input.input_ids.to(config.DEVICE))[0]\n",
    "uncond_input = tokenizer(\n",
    "    [\"\"] * config.BATCH_SIZE, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "      uncond_embeddings = text_encoder(uncond_input.input_ids.to(config.DEVICE))[0]   \n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "print(f'\\033[94mText Embeddings shape: {text_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224fde38",
   "metadata": {},
   "source": [
    "### Setting the latents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eca9c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = torch.randn(\n",
    "  (config.BATCH_SIZE, unet.in_channels, config.HEIGHT // 8, config.WIDTH // 8),\n",
    "  generator=config.GENERATOR,\n",
    ")\n",
    "latents = latents.to(config.DEVICE)\n",
    "\n",
    "print(f'\\033[94mLatent shape: {latents.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa424c27",
   "metadata": {},
   "source": [
    "### Encoding the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18d041e",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler.set_timesteps(config.NUM_INFERENCE_STEPS)\n",
    "latents = latents * scheduler.sigmas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47d3a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "with autocast(config.DEVICE):\n",
    "      for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "        \n",
    "        latent_model_input = torch.cat([latents] * 2)\n",
    "        sigma = scheduler.sigmas[i]\n",
    "        latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
    "\n",
    "        with torch.no_grad():\n",
    "              noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + config.GUIDANCE_SCALE * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        latents = scheduler.step(noise_pred, i, latents).prev_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a769f1",
   "metadata": {},
   "source": [
    "### Decoding the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e825b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = 1 / 0.18215 * latents\n",
    "\n",
    "with torch.no_grad():\n",
    "  image = vae.decode(latents).sample\n",
    "print(f'\\033[94mImage shape: {image.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655793a1",
   "metadata": {},
   "source": [
    "### Vizualising the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96512dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "pil_images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d411ea",
   "metadata": {},
   "source": [
    "## 2nd Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1e4b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = [\"a cybertruck carrying ford mustang on top of it in a F1 race\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92ff22b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_input = tokenizer(prompt, padding=\"max_length\", max_length=tokenizer.model_max_length, truncation=True, return_tensors=\"pt\")\n",
    "max_length = text_input.input_ids.shape[-1]\n",
    "with torch.no_grad():\n",
    "      text_embeddings = text_encoder(text_input.input_ids.to(config.DEVICE))[0]\n",
    "uncond_input = tokenizer(\n",
    "    [\"\"] * config.BATCH_SIZE, padding=\"max_length\", max_length=max_length, return_tensors=\"pt\"\n",
    ")\n",
    "with torch.no_grad():\n",
    "      uncond_embeddings = text_encoder(uncond_input.input_ids.to(config.DEVICE))[0]   \n",
    "text_embeddings = torch.cat([uncond_embeddings, text_embeddings])\n",
    "print(f'\\033[94mText Embeddings shape: {text_embeddings.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305f2402",
   "metadata": {},
   "outputs": [],
   "source": [
    "latents = torch.randn(\n",
    "  (config.BATCH_SIZE, unet.in_channels, config.HEIGHT // 8, config.WIDTH // 8),\n",
    "  generator=config.GENERATOR,\n",
    ")\n",
    "latents = latents.to(config.DEVICE)\n",
    "\n",
    "print(f'\\033[94mLatent shape: {latents.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6264513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding the image\n",
    "scheduler.set_timesteps(config.NUM_INFERENCE_STEPS)\n",
    "latents = latents * scheduler.sigmas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5c772b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with autocast(config.DEVICE):\n",
    "      for i, t in tqdm(enumerate(scheduler.timesteps)):\n",
    "        \n",
    "        latent_model_input = torch.cat([latents] * 2)\n",
    "        sigma = scheduler.sigmas[i]\n",
    "        latent_model_input = latent_model_input / ((sigma**2 + 1) ** 0.5)\n",
    "\n",
    "        with torch.no_grad():\n",
    "              noise_pred = unet(latent_model_input, t, encoder_hidden_states=text_embeddings).sample\n",
    "\n",
    "        noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "        noise_pred = noise_pred_uncond + config.GUIDANCE_SCALE * (noise_pred_text - noise_pred_uncond)\n",
    "\n",
    "        latents = scheduler.step(noise_pred, i, latents).prev_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd03f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoding the image\n",
    "latents = 1 / 0.18215 * latents\n",
    "\n",
    "with torch.no_grad():\n",
    "  image = vae.decode(latents).sample\n",
    "print(f'\\033[94mImage shape: {image.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df80093a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualising the image\n",
    "image = (image / 2 + 0.5).clamp(0, 1)\n",
    "image = image.detach().cpu().permute(0, 2, 3, 1).numpy()\n",
    "images = (image * 255).round().astype(\"uint8\")\n",
    "pil_images = [Image.fromarray(image) for image in images]\n",
    "pil_images[0].save(\"img2.jpg\")\n",
    "pil_images[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324c9fec",
   "metadata": {},
   "source": [
    "### Pretrained Pipeline for Stable Diffusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03fe828",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = StableDiffusionPipeline.from_pretrained(\"CompVis/stable-diffusion-v1-4\", revision=\"fp16\", torch_dtype=torch.float16, use_auth_token=hf_api.token)\n",
    "pipe = pipe.to(config.DEVICE)\n",
    "print(f'\\033[94mStable Diffusion Pipeline created !!!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1b0b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = 4\n",
    "prompt = ['a backward village having fancy cars and solar tops on the rooftops of their huts']\n",
    "with autocast(\"cuda\"):\n",
    "  images = pipe(prompt , num_inference_steps=200).images\n",
    "\n",
    "grid = image_grid(images, rows=2, cols=2)\n",
    "grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
