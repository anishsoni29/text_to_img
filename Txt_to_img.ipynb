{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fab84bc2",
   "metadata": {},
   "source": [
    "## Text to image generation using Stable Diffusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d41bf5",
   "metadata": {},
   "source": [
    "### Installing the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33cb3a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install diffusers==0.3.0 --q\n",
    "!pip install transformers scipy ftfy --q\n",
    "!pip install kaggle --q\n",
    "!pip install \"ipywidgets>=7,<8\" --q\n",
    "import IPython.display "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0da0304",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "72c1c56c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'set_env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 19\u001b[0m\n\u001b[1;32m     15\u001b[0m api_token \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_IQudNZvpotVjTrvEEBXWueJTmXcQkSJhJj\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Set the Hugging Face API token in the environment\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[43mset_env\u001b[49m(api_token\u001b[38;5;241m=\u001b[39mapi_token)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Now you can use Hugging Face APIs without importing kaggle_secrets\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m     23\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m CLIPTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-base-patch32\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'set_env' is not defined"
     ]
    }
   ],
   "source": [
    "import gc\n",
    "import torch\n",
    "from PIL import Image\n",
    "import IPython.display \n",
    "from torch import autocast\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import CLIPTextModel, CLIPTokenizer\n",
    "from diffusers import StableDiffusionPipeline\n",
    "from diffusers import AutoencoderKL, UNet2DConditionModel\n",
    "from diffusers import LMSDiscreteScheduler , PNDMScheduler\n",
    "import os\n",
    "\n",
    "\n",
    "# Set the Hugging Face API token in the environment\n",
    "set_env(api_token=api_token)\n",
    "\n",
    "# Now you can use Hugging Face APIs without importing kaggle_secrets\n",
    "# Example usage\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "model = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "\n",
    "\n",
    "from transformers.hf_api import get_token\n",
    "\n",
    "# Set your Hugging Face API token\n",
    "api_token = \"your_api_token_here\"\n",
    "\n",
    "# Set the API token for Hugging Face\n",
    "os.environ[\"HF_HOME\"] = os.path.join(os.getcwd(), \".huggingface\")\n",
    "get_token(api_token=api_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3c054",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5e10c0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class config : \n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    HEIGHT = 512                        \n",
    "    WIDTH = 512                         \n",
    "    NUM_INFERENCE_STEPS = 500            \n",
    "    GUIDANCE_SCALE = 7.5                \n",
    "    GENERATOR = torch.manual_seed(48)   \n",
    "    BATCH_SIZE = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b983b322",
   "metadata": {},
   "source": [
    "### Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "38694d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_grid(imgs, rows, cols):\n",
    "    assert len(imgs) == rows*cols\n",
    "    w,h = img[0].size\n",
    "    grid = Image.new('RGB', size = (cols*w, rows*h))\n",
    "    grid_w, grid_h = grid.size\n",
    "    for i, img in enumerate(imgs):\n",
    "        grid.paste(img, box=(i%cols*w, i//cols*h))\n",
    "    return grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d87b7b66",
   "metadata": {},
   "source": [
    "### Pipeline -- loading the pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05c922bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Hugging_face' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 4\u001b[0m vae \u001b[38;5;241m=\u001b[39m AutoencoderKL\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompVis/stable-diffusion-v1-4\u001b[39m\u001b[38;5;124m\"\u001b[39m, subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvae\u001b[39m\u001b[38;5;124m\"\u001b[39m, use_auth_token\u001b[38;5;241m=\u001b[39m\u001b[43mHugging_face\u001b[49m)\n\u001b[1;32m      5\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m CLIPTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-large-patch14\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m text_encoder \u001b[38;5;241m=\u001b[39m CLIPTextModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenai/clip-vit-large-patch14\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Hugging_face' is not defined"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "vae = AutoencoderKL.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"vae\", use_auth_token=Hugging_face)\n",
    "tokenizer = CLIPTokenizer.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-large-patch14\")\n",
    "unet = UNet2DConditionModel.from_pretrained(\"CompVis/stable-diffusion-v1-4\", subfolder=\"unet\", use_auth_token=Hugging_face)\n",
    "vae = vae.to(config.DEVICE)\n",
    "text_encoder = text_encoder.to(config.DEVICE)\n",
    "unet = unet.to(config.DEVICE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96875156",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
